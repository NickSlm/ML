{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d731555a-e763-4042-9749-efbf7d443849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 09:57:10.752236: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 09:57:11.139897: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-29 09:57:11.139939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-29 09:57:11.219862: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 09:57:11.390126: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 09:57:11.392909: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 09:57:12.642626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7910b815-39cc-4e10-98e2-190250487abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS_DICT = {\n",
    "    1: \"January\",\n",
    "    2: \"February\",\n",
    "    3: \"March\",\n",
    "    4: \"April\",\n",
    "    5: \"May\",\n",
    "    6: \"June\",\n",
    "    7: \"July\",\n",
    "    8: \"August\",\n",
    "    9: \"September\",\n",
    "    10: \"October\",\n",
    "    11: \"November\",\n",
    "    12: \"December\"\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa6a62e-f5b5-403f-b46f-2501767a247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    X, y = [], []\n",
    "    ordinal_min = date(1000, 1, 1).toordinal()\n",
    "    ordinal_max = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinal_random = np.random.randint(ordinal_max - ordinal_min, size=n_samples) + ordinal_min\n",
    "\n",
    "    for ordinal in ordinal_random:\n",
    "        dt = date.fromordinal(ordinal)\n",
    "        month = MONTHS_DICT[dt.month]\n",
    "        y.append(dt.isoformat())\n",
    "        X.append(month + \" \" + dt.strftime(\"%d, %Y\"))\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d63a4a-362e-4968-8556-3c41e84873f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHARS = sorted(set(\"\".join(MONTHS_DICT.values()) + \"1234567890, \"))\n",
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "def vectorize_input(data):\n",
    "    return [INPUT_CHARS.index(char) for char in data]\n",
    "\n",
    "def vectorize_output(data):\n",
    "    return [OUTPUT_CHARS.index(char) for char in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36728810-2c03-452c-8b42-229e344cfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples, batch_size=32):\n",
    "    x, y = generate_data(n_samples)\n",
    "    \n",
    "    X = [vectorize_input(dt) for dt in x]\n",
    "    Y = [vectorize_output(dt) for dt in y]\n",
    "    X, Y = tf.ragged.constant(X, ragged_rank=1), tf.ragged.constant(Y,ragged_rank=1)\n",
    "\n",
    "    X, Y = (X + 1).to_tensor(), (Y + 1).to_tensor()\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "    # dataset = dataset.shuffle(n_samples)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset, X.shape, Y.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f0f46d-8f8e-4310-82ef-bc5e726ee852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 09:57:20.709400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-29 09:57:20.904870: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, MAX_INPUT_SHAPE, MAX_OUTPUT_SHAPE = create_dataset(n_samples=15000)\n",
    "test_dataset, _, _ = create_dataset(n_samples=3000)\n",
    "val_dataset, _, _ = create_dataset(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60514b9-fdac-4b01-96df-79d1d3e6032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[17 21 33 23 26  1  5  3  2  1  5  8  6  7  0  0  0  0]\n",
      " [16 36 30 24  1  5  4  2  1  8 11  5  5  0  0  0  0  0]\n",
      " [13 32 33 27 28  1  3  4  2  1  4 10  4 11  0  0  0  0]\n",
      " [17 21 33 23 26  1  4  3  2  1 11  6 11  4  0  0  0  0]\n",
      " [16 36 28 38  1  5  9  2  1  5 10  5  4  0  0  0  0  0]\n",
      " [19 23 35 31 22 24 33  1  5 12  2  1  6  8  3  9  0  0]\n",
      " [16 21 30 36 21 33 38  1  3 12  2  1  8  6 10 12  0  0]\n",
      " [13 32 33 27 28  1  3  6  2  1  7  8  5  9  0  0  0  0]\n",
      " [13 32 33 27 28  1  5 12  2  1 10  5  9 10  0  0  0  0]\n",
      " [17 21 33 23 26  1  3  5  2  1  9  4  4  7  0  0  0  0]\n",
      " [14 24 23 24 29 22 24 33  1  3  8  2  1 10  8  6  3  0]\n",
      " [15 24 22 33 36 21 33 38  1  3 11  2  1 10  7  6  5  0]\n",
      " [17 21 33 23 26  1  5 12  2  1  4  7 12  9  0  0  0  0]\n",
      " [20 24 32 35 24 29 22 24 33  1  3 10  2  1  5  4  3  4]\n",
      " [19 23 35 31 22 24 33  1  4  6  2  1 11 12  9  8  0  0]\n",
      " [17 21 33 23 26  1  6  4  2  1  7  9  5 11  0  0  0  0]\n",
      " [16 21 30 36 21 33 38  1  5  8  2  1 11 12 10  6  0  0]\n",
      " [20 24 32 35 24 29 22 24 33  1  5 11  2  1  5  8 12 11]\n",
      " [20 24 32 35 24 29 22 24 33  1  3 10  2  1  5  7 11 11]\n",
      " [17 21 38  1  4  7  2  1  8 10  7  8  0  0  0  0  0  0]\n",
      " [13 36 25 36 34 35  1  5  6  2  1  7 12 10  4  0  0  0]\n",
      " [13 36 25 36 34 35  1  4 12  2  1  7  8  4 11  0  0  0]\n",
      " [16 21 30 36 21 33 38  1  3  9  2  1 12  6  9  4  0  0]\n",
      " [20 24 32 35 24 29 22 24 33  1  5 11  2  1  4  6 11 12]\n",
      " [17 21 38  1  5  7  2  1  7  4  5  8  0  0  0  0  0  0]\n",
      " [16 36 30 24  1  3  7  2  1  4 12  4  4  0  0  0  0  0]\n",
      " [15 24 22 33 36 21 33 38  1  4  4  2  1  5 12  4  8  0]\n",
      " [17 21 33 23 26  1  4 12  2  1 12  3 10  4  0  0  0  0]\n",
      " [16 21 30 36 21 33 38  1  5  6  2  1  9 12  5  3  0  0]\n",
      " [13 36 25 36 34 35  1  5  4  2  1 11  8 10  8  0  0  0]\n",
      " [17 21 33 23 26  1  4  3  2  1 12  9  6  8  0  0  0  0]\n",
      " [14 24 23 24 29 22 24 33  1  4  5  2  1  4 12 12  9  0]], shape=(32, 18), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# DELETE\n",
    "for x,y in train_dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48345486-8501-47b0-8393-7c551591b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=embedding_size, input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")   \n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(MAX_OUTPUT_SHAPE[1]),\n",
    "    decoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2608126a-3e52-4536-844f-6dc04165a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 16s 28ms/step - loss: 1.5509 - accuracy: 0.4436 - val_loss: 1.1259 - val_accuracy: 0.5977\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.9017 - accuracy: 0.6721 - val_loss: 0.6950 - val_accuracy: 0.7362\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.5879 - accuracy: 0.7654 - val_loss: 0.4670 - val_accuracy: 0.8110\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.4844 - accuracy: 0.8160 - val_loss: 0.3264 - val_accuracy: 0.8686\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.2467 - accuracy: 0.9093 - val_loss: 0.1924 - val_accuracy: 0.9356\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.1248 - accuracy: 0.9646 - val_loss: 0.0883 - val_accuracy: 0.9789\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0747 - accuracy: 0.9842 - val_loss: 0.0407 - val_accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0271 - accuracy: 0.9969 - val_loss: 0.0205 - val_accuracy: 0.9980\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0131 - accuracy: 0.9992 - val_loss: 0.0112 - val_accuracy: 0.9991\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.0074 - accuracy: 0.9997 - val_loss: 0.0070 - val_accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb650e7d220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94522f9a-0d51-4222-ad23-cffe8bf98c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(date_strs):\n",
    "    ids = [vectorize_input(str) for str in date_strs]\n",
    "    X = tf.ragged.constant(ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2712963-95df-4351-8d40-5a27fd25ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f218be-a06e-4e3d-b057-c3f8859160f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 452ms/step\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(model.predict(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c054eac-0caf-432e-abea-cc94442fa867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "for id in ids:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb51093e-16a4-4cda-bebb-9f048790955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9258661-de16-4f13-b196-e72a65e2b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 444ms/step\n",
      "2000-02-02\n",
      "1789-09-14\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for id in ids:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83862de6-215f-4084-ad84-dfa1fe104c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"May 02, 2020\", \"September 17, 2009\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071f8e34-df28-4235-9b44-302c47a8d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
       "array([17, 21, 38,  1,  3,  5,  2,  1,  5,  3,  5,  3,  0,  0,  0,  0,  0,\n",
       "        0], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14f003e-f94d-46ef-b371-3d1f778ef3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [vectorize_input(dt) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e26f2d0-181a-455f-8b21-eee8a5688d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < MAX_INPUT_SHAPE[1]:\n",
    "        X = tf.pad(X, [[0, 0], [0, MAX_INPUT_SHAPE[1] - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = np.argmax(model.predict(X), axis=-1)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48741b2e-5f6a-4c76-a039-81914283ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = convert_date_strs([\"July 14, 1789\", \"May 01, 2020\", \"August 01, 1993\", \"November 14, 1996\", \n",
    "                           \"May 22, 4322\", \"May 01, 1999\", \"July 14, 4111\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29328af2-556b-452e-b486-689a9007d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-07-14\n",
      "2020-05-01\n",
      "1993-08-01\n",
      "1996-11-14\n",
      "4322-05-22\n",
      "1999-05-01\n",
      "4111-07-14\n"
     ]
    }
   ],
   "source": [
    "for id in pred:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87503d09-da5b-4d68-ae48-b7648c63560a",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e90d85c7-4e52-4fd6-9035-a73a9ca80d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = len(OUTPUT_CHARS) + 1\n",
    "EOS_TOKEN = SOS_TOKEN + 1\n",
    "def shift_output(dataset):\n",
    "    X = np.concatenate([X for X,Y in dataset], axis=0)\n",
    "    Y = np.concatenate([Y for X,Y in dataset], axis=0)\n",
    "\n",
    "    sos_token = tf.fill(dims=(Y.shape[0], 1), value=SOS_TOKEN)\n",
    "    X_decoder = np.concatenate([sos_token, Y[:, :-1]], axis=1)\n",
    "    \n",
    "    return tf.constant(X), X_decoder, tf.constant(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee79f9d2-8a92-46fe-8847-4057e7ec7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_decoder, y_train = shift_output(train_dataset)\n",
    "X_test, X_test_decoder, y_test = shift_output(test_dataset)\n",
    "X_val, X_val_decoder, y_val = shift_output(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6156a03d-c286-4a04-8f4a-9e4b958f0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 21s 39ms/step - loss: 0.6839 - accuracy: 0.7673 - val_loss: 0.1474 - val_accuracy: 0.9667\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0805 - accuracy: 0.9836 - val_loss: 0.0314 - val_accuracy: 0.9973\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0250 - accuracy: 0.9974 - val_loss: 0.0468 - val_accuracy: 0.9888\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0436 - accuracy: 0.9911 - val_loss: 0.0137 - val_accuracy: 0.9995\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0069 - accuracy: 0.9999 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0255 - accuracy: 0.9948 - val_loss: 0.0119 - val_accuracy: 0.9991\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9999\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0233 - accuracy: 0.9950 - val_loss: 0.0025 - val_accuracy: 0.9999\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 7.8250e-04 - accuracy: 1.0000 - val_loss: 7.4668e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 5.1009e-04 - accuracy: 1.0000 - val_loss: 5.2784e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 3.6097e-04 - accuracy: 1.0000 - val_loss: 3.9005e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 2.6543e-04 - accuracy: 1.0000 - val_loss: 2.9530e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.9822e-04 - accuracy: 1.0000 - val_loss: 2.2367e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.5044e-04 - accuracy: 1.0000 - val_loss: 1.7887e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.1577e-04 - accuracy: 1.0000 - val_loss: 1.4024e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 8.9679e-05 - accuracy: 1.0000 - val_loss: 1.0707e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 6.9580e-05 - accuracy: 1.0000 - val_loss: 8.2400e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "encoder_input_layer = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding_layer = keras.layers.Embedding(input_dim=len(INPUT_CHARS)+ 1, output_dim=512)(encoder_input_layer)\n",
    "output, encoder_h_state, encoder_c_state = keras.layers.LSTM(128, return_state=True)(encoder_embedding_layer)\n",
    "\n",
    "encoder_state = [encoder_h_state, encoder_c_state]\n",
    "\n",
    "decoder_input_layer = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding_layer = keras.layers.Embedding(input_dim=len(OUTPUT_CHARS)+ 2, output_dim=512)(decoder_input_layer)\n",
    "decoder_LSTM_output = keras.layers.LSTM(128, return_sequences=True)(decoder_embedding_layer, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS)+1, activation=\"softmax\")(decoder_LSTM_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], y_train, epochs=20, validation_data=([X_val, X_val_decoder], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b20d873-1b09-4906-95a1-9266ef41d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = [\"July 14, 1789\", \"May 01, 2020\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772fe18b-2127-4135-858d-4607c724f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=[len(X), 1], value=SOS_TOKEN)\n",
    "    for index in range(MAX_OUTPUT_SHAPE[1]):\n",
    "        X_decoder = tf.pad(Y_pred, [[0,0],[0, MAX_OUTPUT_SHAPE[1] - Y_pred.shape[1]]])\n",
    "        pred_indices = np.argmax(model.predict([X, X_decoder])[:,index:index+1], axis=-1)\n",
    "        Y_pred = np.concatenate((Y_pred, pred_indices), axis=1)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78ae8ba3-1b10-498f-b038-3cc5d11f2273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_date_strs([\"July 14, 1789\", \"May 01, 2020\", \"August 01, 1993\", \"November 14, 1996\", \n",
    "                           \"May 22, 4322\", \"May 01, 1999\", \"July 14, 4111\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bfb5e77-fbc6-4d8e-96c7-37546e81a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-07-14\n",
      "2020-05-01\n",
      "1993-08-01\n",
      "1996-11-14\n",
      "4322-05-22\n",
      "1999-05-01\n",
      "4111-07-14\n"
     ]
    }
   ],
   "source": [
    "for dt in y_pred[:,1:]:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in dt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998995c4-5445-459a-b498-a6aa50e51bb1",
   "metadata": {},
   "source": [
    "### TFA seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "847d4173-3071-41a9-a1f5-21eac5404266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb137fe0-0e37-45e3-b871-b709766fbfdd",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cece9cfc-eac5-4c17-a6b8-f05075ca5e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 21s 39ms/step - loss: 0.7107 - accuracy: 0.7545 - val_loss: 0.1536 - val_accuracy: 0.9625\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0720 - accuracy: 0.9870 - val_loss: 0.0422 - val_accuracy: 0.9941\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0206 - accuracy: 0.9985 - val_loss: 0.0099 - val_accuracy: 0.9998\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0396 - accuracy: 0.9927 - val_loss: 0.0090 - val_accuracy: 0.9997\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0626 - accuracy: 0.9868 - val_loss: 0.0075 - val_accuracy: 0.9999\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 0.9989\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 9.3607e-04 - accuracy: 1.0000 - val_loss: 8.7287e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 6.4123e-04 - accuracy: 1.0000 - val_loss: 6.5308e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 4.6011e-04 - accuracy: 1.0000 - val_loss: 4.6502e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 3.3984e-04 - accuracy: 1.0000 - val_loss: 3.6319e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 2.5944e-04 - accuracy: 1.0000 - val_loss: 2.8371e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 1.9612e-04 - accuracy: 1.0000 - val_loss: 2.1968e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0487 - accuracy: 0.9892 - val_loss: 0.0075 - val_accuracy: 0.9991\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 7.2339e-04 - accuracy: 1.0000 - val_loss: 6.2807e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 4.3150e-04 - accuracy: 1.0000 - val_loss: 3.9977e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb5987518e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 512\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding_layer = keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=EMBEDDING_SIZE)\n",
    "encoder_embedding = encoder_embedding_layer(encoder_input)\n",
    "\n",
    "encoder_output, encoder_hidden_state, encoder_cell_state = keras.layers.LSTM(128, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_hidden_state, encoder_cell_state]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding_layer = keras.layers.Embedding(input_dim=len(OUTPUT_CHARS) + 2, output_dim=EMBEDDING_SIZE)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "sampler = tfa.seq2seq.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(128)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS)+1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    cell=decoder_cell,\n",
    "    sampler=sampler,\n",
    "    output_layer=output_layer\n",
    ")\n",
    "\n",
    "final_output, final_state, final_lengths = decoder(decoder_embedding, initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_output.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit([X_train, X_train_decoder], y_train, epochs=20, validation_data=([X_val, X_val_decoder], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b5cf99-e51f-48de-9e96-8a3780a7f515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 406ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9936aa21-77b0-4d1b-a738-29649b6ae2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-07-14\n",
      "2020-05-01\n"
     ]
    }
   ],
   "source": [
    "for dt in y_pred[:,1:]:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in dt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9bf26-367d-4331-a114-48d2eaa5b075",
   "metadata": {},
   "source": [
    "### GreedyEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb3e4f-f2dc-4bcc-ba47-0d960969ced0",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9481e1e-8f38-48be-809a-f53977b96972",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.GreedyEmbeddingSampler(embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    cell=decoder_cell,\n",
    "    sampler=inference_sampler,\n",
    "    output_layer=output_layer,\n",
    "    maximum_iterations= MAX_OUTPUT_SHAPE[1]\n",
    ")\n",
    "batch_size = tf.shape(encoder_input)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=SOS_TOKEN)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e11a0287-4d37-477e-b5fe-b6a06eee7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model =keras.models.Model(inputs=[encoder_input], outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38d814a6-8ee0-4d0e-b943-a0132d08be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b459abb-624d-47ee-b0de-c9ee2eed83af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 367ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445098c2-b94a-44fd-950e-22d011152397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
