{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d731555a-e763-4042-9749-efbf7d443849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 16:06:06.453091: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 16:06:06.485241: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-28 16:06:06.485264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-28 16:06:06.485950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-28 16:06:06.490587: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 16:06:06.491359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 16:06:07.083828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7910b815-39cc-4e10-98e2-190250487abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10\n",
    "\n",
    "MONTHS_DICT = {\n",
    "    1: \"January\",\n",
    "    2: \"February\",\n",
    "    3: \"March\",\n",
    "    4: \"April\",\n",
    "    5: \"May\",\n",
    "    6: \"June\",\n",
    "    7: \"July\",\n",
    "    8: \"August\",\n",
    "    9: \"September\",\n",
    "    10: \"October\",\n",
    "    11: \"November\",\n",
    "    12: \"December\"\n",
    "              }\n",
    "\n",
    "ordinal_min = date(999, 1, 1).toordinal()\n",
    "ordinal_max = date(9999, 12, 31).toordinal()\n",
    "ordinal_random = np.random.randint(ordinal_min, ordinal_max)\n",
    "\n",
    "dt = date.fromordinal(ordinal_random)\n",
    "tm_year, tm_month, tm_day = dt.timetuple().tm_year, dt.timetuple().tm_mon, dt.timetuple().tm_mday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa6a62e-f5b5-403f-b46f-2501767a247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    X, y = [], []\n",
    "    ordinal_min = date(1000, 1, 1).toordinal()\n",
    "    ordinal_max = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinal_random = np.random.randint(ordinal_max - ordinal_min, size=n_samples) + ordinal_min\n",
    "\n",
    "    for ordinal in ordinal_random:\n",
    "        dt = date.fromordinal(ordinal)\n",
    "        tm_year, tm_month, tm_day = dt.timetuple().tm_year, dt.timetuple().tm_mon, dt.timetuple().tm_mday\n",
    "        month = MONTHS_DICT[tm_month]\n",
    "        y.append(dt.isoformat())\n",
    "        X.append(f\"{month} {tm_day}, {tm_year}\")\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d63a4a-362e-4968-8556-3c41e84873f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHARS = sorted(set(\"\".join(MONTHS_DICT.values()) + \"1234567890, \"))\n",
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "def vectorize_input(data):\n",
    "    return [INPUT_CHARS.index(char) for char in data]\n",
    "\n",
    "def vectorize_output(data):\n",
    "    return [OUTPUT_CHARS.index(char) for char in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36728810-2c03-452c-8b42-229e344cfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples, batch_size=32):\n",
    "    x, y = generate_data(n_samples)\n",
    "    \n",
    "    X = [vectorize_input(dt) for dt in x]\n",
    "    Y = [vectorize_output(dt) for dt in y]\n",
    "    X, Y = tf.ragged.constant(X, ragged_rank=1), tf.ragged.constant(Y,ragged_rank=1)\n",
    "\n",
    "    X, Y = (X + 1).to_tensor(), (Y + 1).to_tensor()\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "    dataset = dataset.shuffle(n_samples)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset, X.shape, Y.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f0f46d-8f8e-4310-82ef-bc5e726ee852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 16:06:12.680438: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 16:06:12.698109: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, MAX_INPUT_SHAPE, MAX_OUTPUT_SHAPE = create_dataset(n_samples=15000)\n",
    "test_dataset, _, _ = create_dataset(n_samples=3000)\n",
    "val_dataset, _, _ = create_dataset(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48345486-8501-47b0-8393-7c551591b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=embedding_size, input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")   \n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(MAX_OUTPUT_SHAPE[1]),\n",
    "    decoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2608126a-3e52-4536-844f-6dc04165a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 16s 28ms/step - loss: 1.5716 - accuracy: 0.4357 - val_loss: 1.1568 - val_accuracy: 0.5778\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 1.0690 - accuracy: 0.6147 - val_loss: 0.8792 - val_accuracy: 0.6816\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.7049 - accuracy: 0.7394 - val_loss: 0.5572 - val_accuracy: 0.7872\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.6097 - accuracy: 0.7839 - val_loss: 0.3781 - val_accuracy: 0.8619\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.2712 - accuracy: 0.9070 - val_loss: 0.1795 - val_accuracy: 0.9480\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.1724 - accuracy: 0.9571 - val_loss: 0.0891 - val_accuracy: 0.9816\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.0587 - accuracy: 0.9908 - val_loss: 0.0417 - val_accuracy: 0.9937\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.1504 - accuracy: 0.9651 - val_loss: 0.0828 - val_accuracy: 0.9870\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0391 - accuracy: 0.9965 - val_loss: 0.0241 - val_accuracy: 0.9981\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.0161 - accuracy: 0.9992 - val_loss: 0.0144 - val_accuracy: 0.9990\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0094 - accuracy: 0.9997 - val_loss: 0.0088 - val_accuracy: 0.9997\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0060 - accuracy: 0.9999 - val_loss: 0.0060 - val_accuracy: 0.9998\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9999\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9999\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.1197 - accuracy: 0.9747 - val_loss: 0.0089 - val_accuracy: 0.9998\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9999\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9999\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f287a5b5a00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, epochs=20, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94522f9a-0d51-4222-ad23-cffe8bf98c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(date_strs):\n",
    "    ids = [vectorize_input(str) for str in date_strs]\n",
    "    X = tf.ragged.constant(ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2712963-95df-4351-8d40-5a27fd25ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f218be-a06e-4e3d-b057-c3f8859160f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 513ms/step\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(model.predict(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c054eac-0caf-432e-abea-cc94442fa867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "for id in ids:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb51093e-16a4-4cda-bebb-9f048790955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9258661-de16-4f13-b196-e72a65e2b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 455ms/step\n",
      "2020-08-20\n",
      "1789-12-14\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for id in ids:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83862de6-215f-4084-ad84-dfa1fe104c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"May 02, 2020\", \"September 17, 2009\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071f8e34-df28-4235-9b44-302c47a8d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
       "array([17, 21, 38,  1,  3,  5,  2,  1,  5,  3,  5,  3,  0,  0,  0,  0,  0,\n",
       "        0], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d14f003e-f94d-46ef-b371-3d1f778ef3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [vectorize_input(dt) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e26f2d0-181a-455f-8b21-eee8a5688d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    print(X)\n",
    "    if X.shape[1] < MAX_INPUT_SHAPE[1]:\n",
    "        X = tf.pad(X, [[0, 0], [0, MAX_INPUT_SHAPE[1] - X.shape[1]]])\n",
    "        print(X)\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = np.argmax(model.predict(X), axis=-1)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48741b2e-5f6a-4c76-a039-81914283ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[17 21 38  1  4  4  2  1  5  3  5  3  0  0  0  0  0  0]\n",
      " [20 24 32 35 24 29 22 24 33  1  4 10  2  1  5  3  3 12]], shape=(2, 18), dtype=int32)\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = convert_date_strs([\"May 11, 2020\", \"September 17, 2009\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29328af2-556b-452e-b486-689a9007d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11\n",
      "2009-09-17\n"
     ]
    }
   ],
   "source": [
    "for id in pred:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87503d09-da5b-4d68-ae48-b7648c63560a",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e90d85c7-4e52-4fd6-9035-a73a9ca80d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = len(OUTPUT_CHARS) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee79f9d2-8a92-46fe-8847-4057e7ec7b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 11)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate([x for x, y in train_dataset], axis=0)\n",
    "y_train = np.concatenate([y for x, y in train_dataset], axis=0)\n",
    "sos_tokens = tf.fill([y_train.shape[0], 1], value=token_id)\n",
    "x_train_decoder = tf.concat((sos_tokens, y_train), axis=1)\n",
    "print(x_train_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6156a03d-c286-4a04-8f4a-9e4b958f0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_layer = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding_layer = keras.layers.Embedding(input_dim=len(INPUT_CHARS)+ 1, output_dim=512)(encoder_input_layer)\n",
    "output, encoder_h_state, encoder_c_state = keras.layers.LSTM(128, return_state=True)(encoder_embedding_layer)\n",
    "\n",
    "encoder_state = [encoder_h_state, encoder_c_state]\n",
    "\n",
    "decoder_input_layer = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding_layer = keras.layers.Embedding(input_dim=len(OUTPUT_CHARS)+ 2, output_dim=512)(decoder_input_layer)\n",
    "decoder_output = keras.layers.LSTM(128, return_sequences=True)(decoder_embedding_layer, initial_state=encoder_state)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit([x_train, x_train_decoder], y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788beb40-f916-47b8-923a-50e078344432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
