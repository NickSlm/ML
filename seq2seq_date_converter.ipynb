{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d731555a-e763-4042-9749-efbf7d443849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 16:42:10.982628: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 16:42:11.054537: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-28 16:42:11.054609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-28 16:42:11.060804: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-28 16:42:11.088736: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 16:42:11.089748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 16:42:11.806002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7910b815-39cc-4e10-98e2-190250487abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10\n",
    "\n",
    "MONTHS_DICT = {\n",
    "    1: \"January\",\n",
    "    2: \"February\",\n",
    "    3: \"March\",\n",
    "    4: \"April\",\n",
    "    5: \"May\",\n",
    "    6: \"June\",\n",
    "    7: \"July\",\n",
    "    8: \"August\",\n",
    "    9: \"September\",\n",
    "    10: \"October\",\n",
    "    11: \"November\",\n",
    "    12: \"December\"\n",
    "              }\n",
    "\n",
    "ordinal_min = date(999, 1, 1).toordinal()\n",
    "ordinal_max = date(9999, 12, 31).toordinal()\n",
    "ordinal_random = np.random.randint(ordinal_min, ordinal_max)\n",
    "\n",
    "dt = date.fromordinal(ordinal_random)\n",
    "tm_year, tm_month, tm_day = dt.timetuple().tm_year, dt.timetuple().tm_mon, dt.timetuple().tm_mday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa6a62e-f5b5-403f-b46f-2501767a247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    X, y = [], []\n",
    "    ordinal_min = date(1000, 1, 1).toordinal()\n",
    "    ordinal_max = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinal_random = np.random.randint(ordinal_max - ordinal_min, size=n_samples) + ordinal_min\n",
    "\n",
    "    for ordinal in ordinal_random:\n",
    "        dt = date.fromordinal(ordinal)\n",
    "        tm_year, tm_month, tm_day = dt.timetuple().tm_year, dt.timetuple().tm_mon, dt.timetuple().tm_mday\n",
    "        month = MONTHS_DICT[tm_month]\n",
    "        y.append(dt.isoformat())\n",
    "        X.append(f\"{month} {tm_day}, {tm_year}\")\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d63a4a-362e-4968-8556-3c41e84873f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHARS = sorted(set(\"\".join(MONTHS_DICT.values()) + \"1234567890, \"))\n",
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "def vectorize_input(data):\n",
    "    return [INPUT_CHARS.index(char) for char in data]\n",
    "\n",
    "def vectorize_output(data):\n",
    "    return [OUTPUT_CHARS.index(char) for char in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36728810-2c03-452c-8b42-229e344cfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples, batch_size=32):\n",
    "    x, y = generate_data(n_samples)\n",
    "    \n",
    "    X = [vectorize_input(dt) for dt in x]\n",
    "    Y = [vectorize_output(dt) for dt in y]\n",
    "    X, Y = tf.ragged.constant(X, ragged_rank=1), tf.ragged.constant(Y,ragged_rank=1)\n",
    "\n",
    "    X, Y = (X + 1).to_tensor(), (Y + 1).to_tensor()\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "    dataset = dataset.shuffle(n_samples)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset, X.shape, Y.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f0f46d-8f8e-4310-82ef-bc5e726ee852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 16:42:17.851852: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 16:42:17.926879: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, MAX_INPUT_SHAPE, MAX_OUTPUT_SHAPE = create_dataset(n_samples=15000)\n",
    "test_dataset, _, _ = create_dataset(n_samples=3000)\n",
    "val_dataset, _, _ = create_dataset(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48345486-8501-47b0-8393-7c551591b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=embedding_size, input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")   \n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(MAX_OUTPUT_SHAPE[1]),\n",
    "    decoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2608126a-3e52-4536-844f-6dc04165a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 15s 27ms/step - loss: 1.6284 - accuracy: 0.4194 - val_loss: 1.3489 - val_accuracy: 0.5200\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 1.0580 - accuracy: 0.6191 - val_loss: 1.0888 - val_accuracy: 0.6062\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.7330 - accuracy: 0.7233 - val_loss: 0.5952 - val_accuracy: 0.7686\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.4582 - accuracy: 0.8186 - val_loss: 0.3398 - val_accuracy: 0.8672\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.3258 - accuracy: 0.8854 - val_loss: 0.1981 - val_accuracy: 0.9347\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.1343 - accuracy: 0.9633 - val_loss: 0.0906 - val_accuracy: 0.9805\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.1057 - accuracy: 0.9786 - val_loss: 0.0590 - val_accuracy: 0.9909\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0364 - accuracy: 0.9953 - val_loss: 0.0273 - val_accuracy: 0.9971\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0194 - accuracy: 0.9982 - val_loss: 0.0164 - val_accuracy: 0.9984\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0140 - accuracy: 0.9988 - val_loss: 0.0114 - val_accuracy: 0.9985\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0079 - accuracy: 0.9996 - val_loss: 0.0066 - val_accuracy: 0.9998\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 0.9999\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 0.9999\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 12s 27ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 7.8102e-04 - accuracy: 1.0000 - val_loss: 8.5703e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 5.7475e-04 - accuracy: 1.0000 - val_loss: 6.3899e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 4.2997e-04 - accuracy: 1.0000 - val_loss: 4.9785e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 3.2097e-04 - accuracy: 1.0000 - val_loss: 3.5893e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7efc45adeca0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, epochs=20, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94522f9a-0d51-4222-ad23-cffe8bf98c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(date_strs):\n",
    "    ids = [vectorize_input(str) for str in date_strs]\n",
    "    X = tf.ragged.constant(ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2712963-95df-4351-8d40-5a27fd25ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f218be-a06e-4e3d-b057-c3f8859160f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 446ms/step\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(model.predict(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c054eac-0caf-432e-abea-cc94442fa867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "for id in ids:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb51093e-16a4-4cda-bebb-9f048790955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9258661-de16-4f13-b196-e72a65e2b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 430ms/step\n",
      "2020-02-20\n",
      "1789-09-14\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for id in ids:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83862de6-215f-4084-ad84-dfa1fe104c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_input([\"May 02, 2020\", \"September 17, 2009\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071f8e34-df28-4235-9b44-302c47a8d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
       "array([17, 21, 38,  1,  3,  5,  2,  1,  5,  3,  5,  3,  0,  0,  0,  0,  0,\n",
       "        0], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d14f003e-f94d-46ef-b371-3d1f778ef3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [vectorize_input(dt) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e26f2d0-181a-455f-8b21-eee8a5688d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    print(X)\n",
    "    if X.shape[1] < MAX_INPUT_SHAPE[1]:\n",
    "        X = tf.pad(X, [[0, 0], [0, MAX_INPUT_SHAPE[1] - X.shape[1]]])\n",
    "        print(X)\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = np.argmax(model.predict(X), axis=-1)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48741b2e-5f6a-4c76-a039-81914283ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[17 21 38  1  4  4  2  1  5  3  5  3  0  0  0  0  0  0]\n",
      " [20 24 32 35 24 29 22 24 33  1  4 10  2  1  5  3  3 12]], shape=(2, 18), dtype=int32)\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = convert_date_strs([\"May 11, 2020\", \"September 17, 2009\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29328af2-556b-452e-b486-689a9007d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11\n",
      "2009-09-17\n"
     ]
    }
   ],
   "source": [
    "for id in pred:\n",
    "    print(''.join([OUTPUT_CHARS[index -1] for index in id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87503d09-da5b-4d68-ae48-b7648c63560a",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e90d85c7-4e52-4fd6-9035-a73a9ca80d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = len(OUTPUT_CHARS) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee79f9d2-8a92-46fe-8847-4057e7ec7b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 11)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate([x for x, y in train_dataset], axis=0)\n",
    "y_train = np.concatenate([y for x, y in train_dataset], axis=0)\n",
    "sos_tokens = tf.fill([y_train.shape[0], 1], value=token_id)\n",
    "x_train_decoder = tf.concat((sos_tokens, y_train), axis=1)\n",
    "print(x_train_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6156a03d-c286-4a04-8f4a-9e4b958f0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/utils/metrics_utils.py\", line 969, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 10 and 11 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_1, Cast_2)' with input shapes: [?,10], [?,11].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mNadam()\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_decoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileykmackb0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/src/utils/metrics_utils.py\", line 969, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 10 and 11 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_1, Cast_2)' with input shapes: [?,10], [?,11].\n"
     ]
    }
   ],
   "source": [
    "encoder_input_layer = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding_layer = keras.layers.Embedding(input_dim=len(INPUT_CHARS)+ 1, output_dim=512)(encoder_input_layer)\n",
    "output, encoder_h_state, encoder_c_state = keras.layers.LSTM(128, return_state=True)(encoder_embedding_layer)\n",
    "\n",
    "encoder_state = [encoder_h_state, encoder_c_state]\n",
    "\n",
    "decoder_input_layer = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding_layer = keras.layers.Embedding(input_dim=len(OUTPUT_CHARS)+ 2, output_dim=512)(decoder_input_layer)\n",
    "decoder_output = keras.layers.LSTM(128, return_sequences=True)(decoder_embedding_layer, initial_state=encoder_state)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit([x_train, x_train_decoder], y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788beb40-f916-47b8-923a-50e078344432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
