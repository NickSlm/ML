{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e837a9-18c9-4269-a93a-fcd768608bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:44:21.872779: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 14:44:21.911253: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 14:44:21.911292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 14:44:21.912656: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 14:44:21.919035: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 14:44:21.919908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 14:44:22.545987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd963dd-eb76-4812-a240-22b3e7435ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "REBER_VOCAB = {\n",
    "    0: 'B',\n",
    "    1: 'T',\n",
    "    2: 'S',\n",
    "    3: 'X',\n",
    "    4: 'P',\n",
    "    5: 'V',\n",
    "    6: 'E',\n",
    "              }\n",
    "\n",
    "default_reber = {\n",
    "    0 : [('B', 1)],\n",
    "    1 : [('T', 2), ('P', 3)],\n",
    "    2 : [('S', 2), ('X', 4)],\n",
    "    3 : [('T', 3), ('V', 5)],\n",
    "    4 : [('X', 3), ('S', 6)],\n",
    "    5 : [('P', 4), ('V', 6)],\n",
    "    6 : [('E', None)]\n",
    "         }\n",
    "\n",
    "embedded_reber = {\n",
    "    0 : [('B', 1)], \n",
    "    1 : [('T', 2), ('P', 3)],\n",
    "    2 : [(default_reber, 4)],\n",
    "    3 : [(default_reber, 5)],\n",
    "    4 : [('T', 6)],\n",
    "    5 : [('P', 6)],\n",
    "    6 : [('E', None)]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4f3277-a165-48a5-b97b-c0e6fc8df113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        rnd_index = np.random.randint(len(grammar[state]))\n",
    "        char, state = grammar[state][rnd_index]\n",
    "        if isinstance(char, dict):\n",
    "            char = generate_string(char)\n",
    "        output.append(char)\n",
    "    return ''.join(output)\n",
    "\n",
    "def generate_error_string(grammar):\n",
    "    legal_string = generate_string(grammar)\n",
    "    illegal_string = list(legal_string)\n",
    "    rnd_indices = np.random.randint(len(illegal_string), size=np.random.randint(len(illegal_string)))    \n",
    "    for i in rnd_indices:\n",
    "        illegal_string[i] = REBER_VOCAB[np.random.randint(7)]\n",
    "    return ''.join(illegal_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2aa9d4-5747-4215-b817-55bb466e0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples, legal_perc, illegal_perc):\n",
    "    legal_strings = []\n",
    "    illegal_strings = []\n",
    "\n",
    "    # legal\n",
    "    for _ in range(n_samples * legal_perc // 100):\n",
    "        legal_strings.append(generate_string(embedded_reber))\n",
    "    # Illegal\n",
    "    for _ in range(n_samples * illegal_perc // 100):\n",
    "        illegal_strings.append(generate_error_string(embedded_reber))\n",
    "\n",
    "    return legal_strings, illegal_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f0a1bb-7d77-4376-a950-19e7ab7299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_file(filepath, n_samples, legal_perc=50, illegal_perc=50):\n",
    "\n",
    "    legal_strings, illegal_strings = generate_data(n_samples, legal_perc, legal_perc)\n",
    "    \n",
    "    with open(filepath, 'w') as file:\n",
    "        for legal_string in legal_strings:\n",
    "            file.write(legal_string + ',' + \"legal\")\n",
    "            file.write('\\n')\n",
    "        for illegal_string in illegal_strings:\n",
    "            file.write(illegal_string + ',' + \"illegal\")\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832e6cbf-a36d-4cd5-9a20-49449daecd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join('datasets', 'reber_grammar')\n",
    "file_path_train = os.path.join(dataset_dir, \"reber_strings_train.txt\")\n",
    "file_path_test = os.path.join(dataset_dir, \"reber_strings_test.txt\")\n",
    "\n",
    "N_SAMPLES = 15000\n",
    "train_size = N_SAMPLES * 80 // 100\n",
    "test_size = N_SAMPLES * 20 // 100\n",
    "\n",
    "generate_data_file(file_path_train, train_size)\n",
    "generate_data_file(file_path_test, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72530cd3-b999-4069-8766-11671285bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    data = Path(filepath).read_text()\n",
    "    lines = data.splitlines()\n",
    "    pairs = [line.split(',') for line in lines]\n",
    "    X = np.array([X for X,Y in pairs])\n",
    "    Y = np.array([Y for X,Y in pairs])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e931bf79-cc58-46c4-a084-363b2b258a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(string):\n",
    "    return [list(REBER_VOCAB.keys())[list(REBER_VOCAB.values()).index(char)] for char in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae1d93a-6e2f-41e7-a531-0b94719ff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, label, buffer_size, batch_size=32):\n",
    "        \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7054c5-e982-435d-8f59-18e0505e3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X, Y):\n",
    "    X = tf.ragged.constant([vectorize_data(string) for string in X])\n",
    "    Y = tf.constant([0 if label == \"illegal\" else 1 for label in Y])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3484be-86d6-44e8-8d4f-0a683c2d3f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:44:24.406320: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-18 14:44:24.447552: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = load_data(Path(file_path_train))\n",
    "X_test, Y_test = load_data(Path(file_path_test))\n",
    "\n",
    "X_train, Y_train = process_data(X_train, Y_train)\n",
    "X_test, Y_test = process_data(X_test, Y_test)\n",
    "\n",
    "train_dataset = create_dataset(X_train, Y_train, buffer_size=train_size)\n",
    "test_dataset = create_dataset(X_test, Y_test, buffer_size=test_size)\n",
    "\n",
    "# val_dataset = test_dataset.take(5 * test_size // 100)\n",
    "# test_dataset = test_dataset.skip(5 * test_size // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8bd38bc-aaac-4bbd-804c-a36943ecfcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "375/375 [==============================] - 8s 17ms/step - loss: 0.5688 - accuracy: 0.7408 - val_loss: 0.5045 - val_accuracy: 0.7863\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.4548 - accuracy: 0.8093 - val_loss: 0.3945 - val_accuracy: 0.8533\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.3474 - accuracy: 0.8654 - val_loss: 0.3034 - val_accuracy: 0.8850\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.2697 - accuracy: 0.9053 - val_loss: 0.2438 - val_accuracy: 0.9187\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.2249 - accuracy: 0.9273 - val_loss: 0.2047 - val_accuracy: 0.9340\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.2039 - accuracy: 0.9362 - val_loss: 0.2002 - val_accuracy: 0.9387\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1920 - accuracy: 0.9407 - val_loss: 0.1966 - val_accuracy: 0.9373\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1870 - accuracy: 0.9423 - val_loss: 0.1999 - val_accuracy: 0.9437\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1841 - accuracy: 0.9443 - val_loss: 0.1868 - val_accuracy: 0.9433\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1845 - accuracy: 0.9441 - val_loss: 0.1816 - val_accuracy: 0.9440\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1813 - accuracy: 0.9450 - val_loss: 0.1834 - val_accuracy: 0.9443\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1811 - accuracy: 0.9446 - val_loss: 0.1817 - val_accuracy: 0.9443\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1814 - accuracy: 0.9449 - val_loss: 0.1859 - val_accuracy: 0.9437\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1814 - accuracy: 0.9452 - val_loss: 0.1853 - val_accuracy: 0.9437\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1809 - accuracy: 0.9451 - val_loss: 0.1836 - val_accuracy: 0.9440\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1820 - accuracy: 0.9450 - val_loss: 0.1849 - val_accuracy: 0.9437\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1809 - accuracy: 0.9451 - val_loss: 0.1819 - val_accuracy: 0.9447\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1803 - accuracy: 0.9453 - val_loss: 0.1804 - val_accuracy: 0.9443\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1795 - accuracy: 0.9453 - val_loss: 0.1900 - val_accuracy: 0.9443\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.1819 - accuracy: 0.9452 - val_loss: 0.1797 - val_accuracy: 0.9443\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    keras.layers.Embedding(input_dim=len(REBER_VOCAB), output_dim=512),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-02, momentum=0.95, nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(train_dataset, epochs=20, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e3b1c1a-8fa8-435d-bb20-e7c656955fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "\n",
      "Estimated probability that these are Reber strings:\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 92.39%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 92.47%\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
    "\n",
    "X_test = tf.ragged.constant([vectorize_data(string) for string in test_strings], ragged_rank=1)\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19158e-77fa-42dc-b337-351007e0057c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
