{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472fedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691ebb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.join(\"datasets\", \"acllmdb\")\n",
    "FILE_PATH = os.path.join(\"datasets\", 'aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d9443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dataset(file_path):\n",
    "#     os.makedirs(DIR_PATH, exist_ok=True)\n",
    "#     shutil.unpack_archive(file_path, DIR_PATH)\n",
    "# extract_dataset(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cae201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/acllmdb/aclImdb')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(DIR_PATH) / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64314c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(dir_path):\n",
    "    return [str(file) for file in dir_path.glob(\"*.txt\")]\n",
    "\n",
    "train_neg_data_files = read_files(path / \"train\" / \"neg\")\n",
    "train_pos_data_files = read_files(path / \"train\" / \"pos\")\n",
    "test_neg_data_files = read_files(path / \"test\" / \"neg\")\n",
    "test_pos_data_files = read_files(path / \"test\" / \"pos\")\n",
    "\n",
    "test_neg_data_files = test_neg_data_files[5000:]\n",
    "test_pos_data_files = test_pos_data_files[5000:]\n",
    "val_neg_data_files = test_neg_data_files[:5000]\n",
    "val_pos_data_files = test_pos_data_files[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89203246",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_n = tf.data.TextLineDataset(train_neg_data_files)\n",
    "train_dataset_p = tf.data.TextLineDataset(train_pos_data_files)\n",
    "test_dataset_n = tf.data.TextLineDataset(train_neg_data_files)\n",
    "test_dataset_p = tf.data.TextLineDataset(train_pos_data_files)\n",
    "val_dataset_n = tf.data.TextLineDataset(val_neg_data_files)\n",
    "val_dataset_p = tf.data.TextLineDataset(val_pos_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec34edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(neg_ds, pos_ds, buffer_size=None, batch_size=32):\n",
    "    neg_ds = neg_ds.map(lambda text: (text, 0))\n",
    "    pos_ds = pos_ds.map(lambda text: (text, 1))\n",
    "    dataset = tf.data.Dataset.concatenate(neg_ds, pos_ds)\n",
    "    if buffer_size is not None:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "train_dataset = merge_datasets(train_dataset_n, train_dataset_p, 25000)\n",
    "test_dataset = merge_datasets(test_dataset_n, test_dataset_p)\n",
    "val_dataset = merge_datasets(val_dataset_n, val_dataset_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f71a69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "36c6ddc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 3199), dtype=float32, numpy=\n",
       "array([[ 0., 24., 10., ...,  0.,  0.,  0.],\n",
       "       [ 0., 28., 16., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  6.,  5., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0., 33., 14., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  6.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  3.,  2., ...,  0.,  0.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset.take(1).map(lambda review, label: review)\n",
    "sample = np.concatenate(list(sample.as_numpy_iterator()))\n",
    "\n",
    "layer = keras.layers.TextVectorization()\n",
    "layer.adapt(sample)\n",
    "sample = layer(sample)\n",
    "\n",
    "a = BagOfWords(3200)\n",
    "a.call(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bb629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
