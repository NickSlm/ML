{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjsxs6ZLVZhJZfr3GIRtHd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickSlm/ml1/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pCdUlU7Gm_Om"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.datasets.imdb.load_data()"
      ],
      "metadata": {
        "id": "bh-mRROQCspp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = dataset"
      ],
      "metadata": {
        "id": "RPsr_docGzBO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = X_train[0][:10]"
      ],
      "metadata": {
        "id": "-M_YgHsKG44Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()"
      ],
      "metadata": {
        "id": "jk_-OttSY2A2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_word = dict((id_ + 3,word) for word, id_ in word_index.items())\n",
        "id_to_word [1] = \"[START]\"\n",
        "id_to_word [2] = \"[OOV]\"\n",
        "\" \".join(id_to_word[id_] for id_ in test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NBZUcWQfcysa",
        "outputId": "e9d4ef6b-a0ee-484e-9562-5491ac0bd33f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] this film was just brilliant casting location scenery story'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
      ],
      "metadata": {
        "id": "YAKON2Mpqway"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = info.splits[\"train\"].num_examples\n",
        "test_size = info.splits[\"test\"].num_examples"
      ],
      "metadata": {
        "id": "QClzRpn2uQvS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "  X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "  X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")    # replace all <br>, <br/>\n",
        "  X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "  X_batch = tf.strings.split(X_batch)\n",
        "  return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "metadata": {
        "id": "cRUo9-GVvyhI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "X4YAOqcxKEM3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "for X_batch, y_batch in dataset[\"train\"].batch(32).map(preprocess):\n",
        "  for review in X_batch:\n",
        "    vocab.update(list(review.numpy()))"
      ],
      "metadata": {
        "id": "cp1jS21OLiUU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = 10000\n",
        "truncated_vocab = [word for word, times in vocab.most_common()[:n_words]]"
      ],
      "metadata": {
        "id": "GbDvx0xrL3Zl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = tf.range(len(truncated_vocab), dtype=tf.int64)\n",
        "words = tf.constant(truncated_vocab)\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, oov_buckets)"
      ],
      "metadata": {
        "id": "QTYB2N5AY8x5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
      ],
      "metadata": {
        "id": "6mKoomvcioPS",
        "outputId": "691bc111-e09c-4720-eb94-b092556cc82d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "  return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = dataset[\"train\"].batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)"
      ],
      "metadata": {
        "id": "YiSfBjip9fIS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(n_words + oov_buckets, 128, mask_zero = True, input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "DBXrhuDeBL52"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reusing Pretrained Embedding"
      ],
      "metadata": {
        "id": "okB_ADkc1zuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "zRE1sMYeNF4X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
        "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "E3TzWr3t2JIG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "batch_size = 32\n",
        "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "iE4APKU-2XU4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "WvZ2AUd18BsR",
        "outputId": "5c8435c1-0674-4a74-9f51-e89b8e830007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 13s 14ms/step - loss: 0.5472 - accuracy: 0.7266\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.5145 - accuracy: 0.7466\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.5093 - accuracy: 0.7508\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.5057 - accuracy: 0.7533\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.5027 - accuracy: 0.7564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lx7hVUfS8KNS"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}