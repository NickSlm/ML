{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5761e073-2fc5-4c99-bf3a-bf8a7c90d617",
   "metadata": {},
   "source": [
    "#### Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf91a17-1bff-4c70-b2ee-f4d848dea3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:12:52.117597: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 14:12:53.011665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-12-09 14:12:53.011751: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-12-09 14:12:53.011759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gymnasium\n",
    "\n",
    "import ale_py\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from livelossplot import PlotLossesKeras, PlotLosses\n",
    "\n",
    "from collections import deque\n",
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bbcfc1-55af-47c8-94e3-27438c6058b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"LunarLander-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acedbb91-a951-411f-a353-ff85fd2daa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "optimizer = keras.optimizers.Adam(learning_rate=6e-3)\n",
    "\n",
    "n_iterations = 500\n",
    "update_period = 16\n",
    "n_max_steps = 1000\n",
    "discount_rate = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28762308-fad2-465b-8b2f-19a127c228dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = env.observation_space.shape[0]\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"relu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(n_outputs, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37755190-d145-4d5e-acec-e05cae91c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(obs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probas = model(obs[np.newaxis])\n",
    "        logits = tf.math.log(probas + keras.backend.epsilon())\n",
    "        action = tf.random.categorical(logits, num_samples=1)\n",
    "        loss = loss_fn(action, probas)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action[0,0].numpy())\n",
    "    done = terminated or truncated\n",
    "    return grads, reward, done, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264575d9-f753-466c-a34b-97e8b1f42e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_steps):\n",
    "    all_grads = []\n",
    "    all_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_grads = []\n",
    "        current_rewards = []\n",
    "        obs, _ = env.reset()\n",
    "        for step in range(n_steps):\n",
    "            grads, reward, done, obs = play_one_step(obs)\n",
    "            current_grads.append(grads)\n",
    "            current_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        all_grads.append(current_grads)\n",
    "        all_rewards.append(current_rewards)\n",
    "    return all_grads, all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03a1dd9e-b37d-4fba-9494-3330bf94af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "    \n",
    "\n",
    "def discount_normalize(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) \n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd5efe-a197-44c1-b7c3-7a74817b2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_grads, all_rewards = play_multiple_episodes(env, update_period, n_max_steps)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
