{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NickSlm/ml1/blob/main/LSTM_Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GSyIPy_8V7w1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 10:53:58.252234: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-03 10:53:58.619254: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-03 10:53:58.619341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-03 10:53:58.697087: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-03 10:53:58.858918: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-03 10:53:58.860784: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 10:54:00.057791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import zipfile, requests, io\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(filepath, target_dir):\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_dir)\n",
    "\n",
    "def move_and_extract(zip_name, src_dir, dst_dir):\n",
    "    if zip_name in os.listdir(dst_dir):\n",
    "        unzip_file(os.path.join(dst_dir, zip_name), dst_dir)\n",
    "        return Path(os.path.join(dst_dir, \"heb.txt\")).parent/\"heb.txt\"\n",
    "        \n",
    "    assert zip_name in os.listdir(src_dir)\n",
    "    shutil.move(os.path.join(src_dir, zip_name), dst_dir)\n",
    "    unzip_file(os.path.join(dst_dir, zip_name), dst_dir)\n",
    "    return Path(os.path.join(dst_dir, \"heb.txt\")).parent/\"heb.txt\"\n",
    "    \n",
    "# def download_and_extract(zipname, url, dst_dir):\n",
    "#     r = requests.get(url)\n",
    "#     if r.status_code == 200:\n",
    "#         print(r.status_code)\n",
    "        # z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        # z.extractall(dst_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_NAME = \"heb-eng.zip\"\n",
    "dataset_dir = os.path.join(\"datasets\", \"sentences\")\n",
    "src_dir = os.path.abspath(\"/mnt/c/Users/nick/\")\n",
    "\n",
    "path_to_file = move_and_extract(ZIP_NAME, src_dir, dataset_dir)\n",
    "# ZIP_NAME = \"spa-eng.zip\"\n",
    "# download_url = \"https://www.manythings.org/anki/spa-eng.zip\"\n",
    "# download_and_extract(ZIP_NAME, download_url, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split(\"\\t\") for line in lines]\n",
    "    context = np.array([context for target, context, _ in pairs])\n",
    "    target = np.array([target for target, context, _  in pairs])\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleanup(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(raw_context, raw_target, buffer_size, batch_size=32):\n",
    "    train_size = np.random.uniform(0, 1, buffer_size) < 0.8\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((raw_context[train_size], raw_target[train_size]))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size)\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((raw_context[~train_size], raw_target[~train_size]))\n",
    "    test_dataset = test_dataset.shuffle(buffer_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(text):\n",
    "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, '[^ a-zA-Zא-ת.?!,¿]', '')\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join(['[SOS]', text, '[EOS]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 10:54:03.596127: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-03 10:54:03.772089: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\xd7\\x99\\xd7\\xa9 \\xd7\\x9c\\xd7\\x96\\xd7\\x94 \\xd7\\xa2\\xd7\\xa8\\xd7\\x9a \\xd7\\xa2\\xd7\\xa6\\xd7\\x95\\xd7\\x9d.'\n",
      " b'\\xd7\\x90\\xd7\\xaa \\xd7\\x9e\\xd7\\x93\\xd7\\x91\\xd7\\xa8\\xd7\\xaa \\xd7\\xa2\\xd7\\x9c\\xd7\\x99\\xd7\\x99?'\n",
      " b'\\xd7\\x97\\xd7\\xa8\\xd7\\x98\\xd7\\x94 \\xd7\\x90\\xd7\\x99\\xd7\\xa0\\xd7\\x94 \\xd7\\x9e\\xd7\\x99\\xd7\\x9c\\xd7\\x94 \\xd7\\xa0\\xd7\\xa8\\xd7\\x93\\xd7\\xa4\\xd7\\xaa \\xd7\\x9c\\xd7\\x94\\xd7\\x91\\xd7\\xa2\\xd7\\xaa \\xd7\\xa6\\xd7\\xa2\\xd7\\xa8.'\n",
      " b'\\xd7\\x94\\xd7\\x9d \\xd7\\x90\\xd7\\x9b\\xd7\\x9c\\xd7\\x95 \\xd7\\x90\\xd7\\xa8\\xd7\\x95\\xd7\\x97\\xd7\\xaa \\xd7\\xa2\\xd7\\xa8\\xd7\\x91.'\n",
      " b'\\xd7\\x90\\xd7\\xa0\\xd7\\x99 \\xd7\\x9c\\xd7\\x90 \\xd7\\x97\\xd7\\x95\\xd7\\xa9\\xd7\\x91 \\xd7\\xa9\\xd7\\x90\\xd7\\xa0\\xd7\\x95 \\xd7\\x9e\\xd7\\xaa\\xd7\\x90\\xd7\\x99\\xd7\\x9e\\xd7\\x99\\xd7\\x9d \\xd7\\x90\\xd7\\x97\\xd7\\x93 \\xd7\\x9c\\xd7\\xa9\\xd7\\xa0\\xd7\\x99 \\xd7\\x99\\xd7\\x95\\xd7\\xaa\\xd7\\xa8.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "raw_context, raw_target = load_data(path_to_file)\n",
    "train_dataset, test_dataset = create_dataset(raw_context, raw_target, len(raw_context) , batch_size=32)\n",
    "\n",
    "for example_context_strings, example_target_strings in train_dataset.take(1):\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_preprocess = tf.keras.layers.TextVectorization(\n",
    "    standardize=data_preprocess,\n",
    "    max_tokens=10000,\n",
    "    ragged=True\n",
    ")\n",
    "target_preprocess = tf.keras.layers.TextVectorization(\n",
    "    standardize=data_preprocess,\n",
    "    max_tokens=10000,\n",
    "    ragged=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOS] אני לא חושב שאנו מתאימים אחד לשני יותר . [EOS]\n"
     ]
    }
   ],
   "source": [
    "context_preprocess.adapt(train_dataset.map(lambda x, y: x))\n",
    "target_preprocess.adapt(train_dataset.map(lambda x, y: y))\n",
    "\n",
    "example_tokens = target_preprocess(example_target_strings)\n",
    "\n",
    "target_vocab = np.array(target_preprocess.get_vocabulary())\n",
    "tokens = target_vocab[example_tokens[4].numpy()]\n",
    "print(' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = context_preprocess(context).to_tensor()\n",
    "    target = target_preprocess(target)\n",
    "    target_in = target[:,:-1].to_tensor()\n",
    "    target_out = target[:,1:].to_tensor()\n",
    "    return (context, target_in), target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(process_text, tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   2   25  196   12   75 5748   21   36   75 1532    4    3    0    0\n",
      "    0    0], shape=(16,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (context, target_in), target_out in train_dataset.take(1):\n",
    "    print(context[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "# decoder_input = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "# sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "# embedding = keras.layers.Embedding(vocab_size, embed_size)\n",
    "# encoder_embedding = embedding(encoder_input)\n",
    "# decoder_embedding = embedding(decoder_input)\n",
    "\n",
    "# encoder = keras.layers.LSTM(512, return_state=True)\n",
    "# encoder_output, hidden_state, cell_state = encoder(encoder_embedding)\n",
    "# encoder_state = [hidden_state, cell_state]\n",
    "\n",
    "# sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "# decoder_cell = keras.layers.LSTMCell(512)\n",
    "# output_layer = keras.layers.Dense(vocab_size)\n",
    "\n",
    "# decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "\n",
    "# final_outputs, final_state, final_sequence_lengths  = decoder(decoder_embedding, initial_state=encoder_state, sequence_length=sequence_lengths)\n",
    "# Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "# model = keras.Model(inputs=[encoder_input, decoder_input, sequence_lengths],outputs=[Y_proba])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiDts3eI4aO+P3lMPfPfvo",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
