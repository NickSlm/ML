{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472fedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691ebb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.join(\"datasets\", \"acllmdb\")\n",
    "FILE_PATH = os.path.join(\"datasets\", 'aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d9443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dataset(file_path):\n",
    "#     os.makedirs(DIR_PATH, exist_ok=True)\n",
    "#     shutil.unpack_archive(file_path, DIR_PATH)\n",
    "# extract_dataset(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cae201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/acllmdb/aclImdb')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(DIR_PATH) / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64314c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(dir_path):\n",
    "    return [str(file) for file in dir_path.glob(\"*.txt\")]\n",
    "\n",
    "train_neg_data_files = read_files(path / \"train\" / \"neg\")\n",
    "train_pos_data_files = read_files(path / \"train\" / \"pos\")\n",
    "test_neg_data_files = read_files(path / \"test\" / \"neg\")\n",
    "test_pos_data_files = read_files(path / \"test\" / \"pos\")\n",
    "\n",
    "test_neg_data_files = test_neg_data_files[5000:]\n",
    "test_pos_data_files = test_pos_data_files[5000:]\n",
    "val_neg_data_files = test_neg_data_files[:5000]\n",
    "val_pos_data_files = test_pos_data_files[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89203246",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_n = tf.data.TextLineDataset(train_neg_data_files)\n",
    "train_dataset_p = tf.data.TextLineDataset(train_pos_data_files)\n",
    "test_dataset_n = tf.data.TextLineDataset(train_neg_data_files)\n",
    "test_dataset_p = tf.data.TextLineDataset(train_pos_data_files)\n",
    "val_dataset_n = tf.data.TextLineDataset(val_neg_data_files)\n",
    "val_dataset_p = tf.data.TextLineDataset(val_pos_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec34edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(neg_ds, pos_ds, buffer_size=None, batch_size=32):\n",
    "    neg_ds = neg_ds.map(lambda text: (text, 0))\n",
    "    pos_ds = pos_ds.map(lambda text: (text, 1))\n",
    "    dataset = tf.data.Dataset.concatenate(neg_ds, pos_ds)\n",
    "    if buffer_size is not None:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "train_dataset = merge_datasets(train_dataset_n, train_dataset_p, 25000)\n",
    "test_dataset = merge_datasets(test_dataset_n, test_dataset_p)\n",
    "val_dataset = merge_datasets(val_dataset_n, val_dataset_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93094bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e46be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1547e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batches = train_dataset.take(100).map(lambda x,y: x)\n",
    "sample = np.concatenate(list(sample_batches.as_numpy_iterator()))\n",
    "\n",
    "VOCAB_SIZE = 1000\n",
    "bag_of_words = BagOfWords(VOCAB_SIZE)\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\", \n",
    "    max_tokens=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3059a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    keras.layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                           output_dim=20,\n",
    "                           mask_zero=True),\n",
    "    keras.layers.Lambda(compute_mean_embedding),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28bc8eed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 54s 62ms/step - loss: 0.4600 - accuracy: 0.7783 - val_loss: 0.4105 - val_accuracy: 0.8038\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 13s 10ms/step - loss: 0.3327 - accuracy: 0.8577 - val_loss: 0.3288 - val_accuracy: 0.8574\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 13s 11ms/step - loss: 0.3171 - accuracy: 0.8637 - val_loss: 0.3200 - val_accuracy: 0.8620\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 13s 10ms/step - loss: 0.3137 - accuracy: 0.8637 - val_loss: 0.3189 - val_accuracy: 0.8605\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 13s 10ms/step - loss: 0.3083 - accuracy: 0.8669 - val_loss: 0.3196 - val_accuracy: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227c2712e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d630bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.constant([\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1af1afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.27851468]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1656b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.96673506]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = tf.constant([\"If you like adult comedy cartoons, like South Park, then this is nearly a similar format about the small adventures of three teenage girls at Bromwell High. Keisha, Natella and Latrina have given exploding sweets and behaved like bitches, I think Keisha is a good leader. There are also small stories going on with the teachers of the school. There's the idiotic principal, Mr. Bip, the nervous Maths teacher and many others. The cast is also fantastic, Lenny Henry's Gina Yashere, EastEnders Chrissie Watts, Tracy-Ann Oberman, Smack The Pony's Doon Mackichan, Dead Ringers' Mark Perry and Blunder's Nina Conti. I didn't know this came from Canada, but it is very good. Very good!\"])\n",
    "model.predict(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6add4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
