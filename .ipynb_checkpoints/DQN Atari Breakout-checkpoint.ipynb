{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a00a08a-1ab8-46c0-8564-7d389c70d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 15:01:05.440100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 15:01:07.060035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-11-19 15:01:07.060140: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-11-19 15:01:07.060148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit\n",
    "from collections import deque\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909982d5-357a-42a3-b658-2ec2cf52873a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf58f29-3045-47d4-b4fb-34390c385bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariPreprocessingFire(AtariPreprocessing):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, reset_info = super().reset(**kwargs)\n",
    "        super().step(1)\n",
    "        return obs, reset_info\n",
    "    def step(self, action):\n",
    "        self.lives_before_action = self.ale.lives()\n",
    "        obs, rewards, terminated, truncated, info = super().step(action)\n",
    "        done = terminated or truncated\n",
    "        if not done and self.ale.lives() < self.lives_before_action:\n",
    "            super().step(1)\n",
    "        return obs, rewards, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506bb123-c230-402a-8c31-0004aa0c09f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "env = AtariPreprocessingFire(env)\n",
    "env = FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde1ef87-7f82-4837-96e5-fe630d791e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observation(obs):\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[:3]\n",
    "    current_frame_delta = np.maximum(obs[3] - obs[:3].mean(axis=0), 0.)\n",
    "    img[0] += current_frame_delta\n",
    "    img[2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    img = np.transpose(img, (1,2,0))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b370ebd3-2a9f-4966-902c-7c1b24350104",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_period = 4\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=1.0,\n",
    "                                          decay_steps=250000 // update_period,\n",
    "                                          end_learning_rate=0.01)\n",
    "loss_fn = keras.losses.huber\n",
    "discount_factor = 0.99\n",
    "replay_buffer = deque(maxlen=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0efbf55-b8e8-4e9d-b5d5-b6bd7bd7253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=env.observation_space.shape),\n",
    "    keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.),\n",
    "    keras.layers.Conv2D(32, (8,8), strides=4, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Conv2D(64, (4,4), strides=2, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Conv2D(64, (3,3), strides=1, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(4)\n",
    "])\n",
    "\n",
    "target_net = keras.models.clone_model(q_net)\n",
    "target_net.set_weights(q_net.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78afd72a-5066-47b5-824f-daaee65fe94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(obs, action_space, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return action_space.sample()\n",
    "    else:\n",
    "        q_values = q_net.predict(obs[np.newaxis])\n",
    "        return np.argmax(q_values[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b0a32b-9683-47a6-87cd-0085228ab3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DqnAgent:\n",
    "    def __init__(self, q_network, target_network, replay_buffer, discount_factor ,**kwargs):\n",
    "        self.n_train_step = 0\n",
    "        self.q_network = q_network\n",
    "        self.target_network = target_network\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        # metrics\n",
    "        self.episodes = 0\n",
    "        self.environment_steps = 0\n",
    "        \n",
    "    \n",
    "    def initialization(self, num_steps):\n",
    "        \"\"\"\n",
    "        Collect the initial experiences, before training\n",
    "        \"\"\"\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(num_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "\n",
    "    def train_step(self):\n",
    "        rnd_indices = np.random.randint(0, len(self.replay_buffer), 32)\n",
    "        samples = [self.replay_buffer[index] for index in rnd_indices]\n",
    "        states, actions, rewards, next_states, dones = [np.array([sample[index_field] for sample in samples]) \n",
    "                                                        for index_field in range(5)]\n",
    "\n",
    "        target_q_value = self.q_network.predict(next_states)\n",
    "        target_mask = tf.one_hot(np.argmax(target_q_value, axis=1), 4).numpy()\n",
    "        max_target_q_value = (self.target_network.predict(next_states) * target_mask).sum(axis=1)\n",
    "        max_target_q_value = (max_target_q_value * self.discount_factor * (1 - dones) + rewards)\n",
    "        max_target_q_value = max_target_q_value.reshape(-1,1)\n",
    "        q_mask = tf.one_hot(actions, 4)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_value = self.q_network(states)\n",
    "            # We use the tf.reduce_sum instead of .sum() because self.q_network(states)\n",
    "            # outputs a tensor object and not an array\n",
    "            q_value = tf.reduce_sum(q_value * q_mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(loss_fn(max_target_q_value, q_value))\n",
    "\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        grad_norm = tf.reduce_mean([tf.norm(g) for g in grads if g is not None]).numpy()\n",
    "        optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "        return loss, grad_norm\n",
    "        \n",
    "    def collect_step(self, state, env, update_period=4):\n",
    "        \n",
    "        epsilon = epsilon_fn(self.n_train_step)\n",
    "        \n",
    "        for step in range(update_period):\n",
    "            self.environment_steps += 1\n",
    "            \n",
    "            action = epsilon_greedy_policy(state, env.action_space, epsilon)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                self.episodes += 1\n",
    "\n",
    "        loss, grad = self.train_step()\n",
    "        self.n_train_step += 1\n",
    "        if self.n_train_step % 2000 == 0:\n",
    "            self.target_network.set_weights(self.q_network.get_weights())\n",
    "        epsilon = epsilon_fn(self.n_train_step)\n",
    "        return state, loss, grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f661dc37-2765-42bc-9299-126227e7b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DqnAgent(q_network=q_net,\n",
    "                target_network= target_net,\n",
    "                replay_buffer=replay_buffer,\n",
    "                discount_factor=discount_factor\n",
    "                )\n",
    "agent.initialization(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef293a33-aa8d-449b-8f78-0ea5cb99b7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    state, _ = env.reset()\n",
    "    for iteration in range(n_iterations):\n",
    "        state, loss, grad = agent.collect_step(state, env, update_period)\n",
    "    \n",
    "        if iteration % 1000 == 0:\n",
    "            print(f\"\\nNumberOfEpisodes = {agent.episodes} \\nEnvironmentSteps = {agent.environment_steps} \\nLoss = {loss} \\nGrad = {grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8c8dcbb-9d50-4879-aa54-affadbcdf27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fd5841cd1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fd5841cd1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fd5841cd1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fd5841cd1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NumberOfEpisodes = 0 \n",
      "EnvironmentSteps = 4 \n",
      "Loss = 0.0014367670519277453 \n",
      "Grad = 0.024869462475180626\n",
      "\n",
      "NumberOfEpisodes = 23 \n",
      "EnvironmentSteps = 4004 \n",
      "Loss = 0.00027920398861169815 \n",
      "Grad = 0.003786565735936165\n",
      "\n",
      "NumberOfEpisodes = 47 \n",
      "EnvironmentSteps = 8004 \n",
      "Loss = 0.01563822664320469 \n",
      "Grad = 0.007403770927339792\n",
      "\n",
      "NumberOfEpisodes = 68 \n",
      "EnvironmentSteps = 12004 \n",
      "Loss = 2.3532706109108403e-05 \n",
      "Grad = 0.0009659582865424454\n",
      "\n",
      "NumberOfEpisodes = 92 \n",
      "EnvironmentSteps = 16004 \n",
      "Loss = 0.015690933912992477 \n",
      "Grad = 0.013555562123656273\n",
      "\n",
      "NumberOfEpisodes = 113 \n",
      "EnvironmentSteps = 20004 \n",
      "Loss = 0.0008882389520294964 \n",
      "Grad = 0.011536437086760998\n",
      "\n",
      "NumberOfEpisodes = 140 \n",
      "EnvironmentSteps = 24004 \n",
      "Loss = 0.0010331339435651898 \n",
      "Grad = 0.01952582597732544\n",
      "\n",
      "NumberOfEpisodes = 161 \n",
      "EnvironmentSteps = 28004 \n",
      "Loss = 9.87530656857416e-05 \n",
      "Grad = 0.0036428156308829784\n",
      "\n",
      "NumberOfEpisodes = 185 \n",
      "EnvironmentSteps = 32004 \n",
      "Loss = 0.00012651144061237574 \n",
      "Grad = 0.004089357331395149\n",
      "\n",
      "NumberOfEpisodes = 208 \n",
      "EnvironmentSteps = 36004 \n",
      "Loss = 6.0687925724778324e-05 \n",
      "Grad = 0.003503022715449333\n",
      "\n",
      "NumberOfEpisodes = 229 \n",
      "EnvironmentSteps = 40004 \n",
      "Loss = 0.0007138592773117125 \n",
      "Grad = 0.01899947226047516\n",
      "\n",
      "NumberOfEpisodes = 253 \n",
      "EnvironmentSteps = 44004 \n",
      "Loss = 5.139537825016305e-05 \n",
      "Grad = 0.0035326085053384304\n",
      "\n",
      "NumberOfEpisodes = 277 \n",
      "EnvironmentSteps = 48004 \n",
      "Loss = 0.002516820328310132 \n",
      "Grad = 0.06804008781909943\n",
      "\n",
      "NumberOfEpisodes = 298 \n",
      "EnvironmentSteps = 52004 \n",
      "Loss = 5.72035314689856e-05 \n",
      "Grad = 0.0044715143740177155\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(n_iterations)\u001b[0m\n\u001b[1;32m      5\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m----> 7\u001b[0m     state, loss, grad \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_period\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNumberOfEpisodes = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnvironmentSteps = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39menvironment_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGrad = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 73\u001b[0m, in \u001b[0;36mDqnAgent.collect_step\u001b[0;34m(self, state, env, update_period)\u001b[0m\n\u001b[1;32m     70\u001b[0m         state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 73\u001b[0m loss, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_train_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_train_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mDqnAgent.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m target_q_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network\u001b[38;5;241m.\u001b[39mpredict(next_states)\n\u001b[1;32m     38\u001b[0m target_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(np\u001b[38;5;241m.\u001b[39margmax(target_q_value, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 39\u001b[0m max_target_q_value \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m target_mask)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m max_target_q_value \u001b[38;5;241m=\u001b[39m (max_target_q_value \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m+\u001b[39m rewards)\n\u001b[1;32m     41\u001b[0m max_target_q_value \u001b[38;5;241m=\u001b[39m max_target_q_value\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/engine/training.py:2346\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2344\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2345\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2348\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/keras/engine/data_adapter.py:1304\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1304\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1306\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:703\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 703\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:742\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    740\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    741\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 742\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f115327-e4e4-4a81-bcf8-f5711a87698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.show()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46292e48-a6e2-482a-9096-c5b58204158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "state, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = q_net.predict(state[np.newaxis])\n",
    "    state, rewards, terminated, truncated, info = env.step(np.argmax(action))\n",
    "    img = env.render()\n",
    "    done = terminated or truncated\n",
    "    frames.append(img)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcf3bd-a926-47aa-ae66-b2284ec65324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb5be1-d3fa-468a-bb5b-3271f48c91d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
