{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a00a08a-1ab8-46c0-8564-7d389c70d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:10:08.568680: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 16:10:10.164476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-11-12 16:10:10.164573: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-11-12 16:10:10.164582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "from gymnasium.wrappers import FrameStackObservation, TimeLimit\n",
    "from collections import deque\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909982d5-357a-42a3-b658-2ec2cf52873a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf58f29-3045-47d4-b4fb-34390c385bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariPreprocessingFire(AtariPreprocessing):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, reset_info = super().reset(**kwargs)\n",
    "        super().step(1)\n",
    "        return obs, reset_info\n",
    "    def step(self, action):\n",
    "        self.lives_before_action = self.ale.lives()\n",
    "        obs, rewards, terminated, truncated, info = super().step(action)\n",
    "        done = terminated or truncated\n",
    "        if not done and self.ale.lives() < self.lives_before_action:\n",
    "            super().step(1)\n",
    "        return obs, rewards, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506bb123-c230-402a-8c31-0004aa0c09f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "env = AtariPreprocessingFire(env)\n",
    "env = FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde1ef87-7f82-4837-96e5-fe630d791e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observation(obs):\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[:3]\n",
    "    current_frame_delta = np.maximum(obs[3] - obs[:3].mean(axis=0), 0.)\n",
    "    img[0] += current_frame_delta\n",
    "    img[2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    img = np.transpose(img, (1,2,0))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b370ebd3-2a9f-4966-902c-7c1b24350104",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_period = 4\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0, epsilon=1e-5, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=1.0,\n",
    "                                          decay_steps=250000 // update_period,\n",
    "                                          end_learning_rate=0.01)\n",
    "replay_buffer = deque(maxlen=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0efbf55-b8e8-4e9d-b5d5-b6bd7bd7253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=env.observation_space.shape),\n",
    "    keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.),\n",
    "    keras.layers.Conv2D(32, (8,8), strides=4, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Conv2D(64, (4,4), strides=2, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Conv2D(64, (3,3), strides=1, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(4)\n",
    "])\n",
    "\n",
    "target_net = keras.models.clone_model(q_net)\n",
    "target_net.set_weights(q_net.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78afd72a-5066-47b5-824f-daaee65fe94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(obs, action_space, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return action_space.sample()\n",
    "    else:\n",
    "        q_values = q_net.predict(obs)\n",
    "        return np.argmax(q_values)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b0a32b-9683-47a6-87cd-0085228ab3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "class DqnAgent:\n",
    "    def __init__(self, n_iterations, q_network, **kwargs):\n",
    "        self.n_train_step = 0\n",
    "        self.n_iterations = n_iterations\n",
    "        self.q_network = q_network\n",
    "\n",
    "        # metrics\n",
    "        self.episodes = 0\n",
    "        self.environment_steps = 0\n",
    "        \n",
    "    \n",
    "    def initialization(self, num_steps, replay_buffer):\n",
    "        \"\"\"\n",
    "        Collect the initial experiences, before training\n",
    "        \"\"\"\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(num_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "\n",
    "    def train_step(self):\n",
    "        pass\n",
    "            \n",
    "    def collect_step(self, replay_buffer, update_period=4):\n",
    "        state, _ = env.reset()\n",
    "        epsilon = epsilon_fn(self.train_step)\n",
    "        \n",
    "        for step in range(update_period):\n",
    "            \n",
    "            action = epsilon_greedy_policy(state, env.action_space, epsilon)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                self.episodes += 1\n",
    "\n",
    "        self.train_step()\n",
    "        self.n_train_step += 1\n",
    "        epsilon = epsilon_fn(self.train_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f661dc37-2765-42bc-9299-126227e7b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DqnAgent(n_iterations=50000,\n",
    "                max_episode_steps=27000,\n",
    "                q_network=q_net,\n",
    "                )\n",
    "agent.initialization(20000, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef293a33-aa8d-449b-8f78-0ea5cb99b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    agent.collect_step(replay_buffer, update_period)\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        print(f\"NumberOfEpisodes = {agent.episodes} \n",
    "        \\nEnvironmentSteps = {agent.environment_steps} \n",
    "        \\nAverageReturn = {} \n",
    "        \\nAverageEpisodeLength = {}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1f2a1-5530-41db-b335-63b05e9ce52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
